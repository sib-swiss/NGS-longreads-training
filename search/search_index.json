{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Course website Material This website Zoom meeting (through mail) Google doc (through mail) Slack channel Learning outcomes After this course, you will be able to: Understand the basics behind PacBio SMRT sequencing and Oxford Nanopore Technology sequencing Use the command line to perform quality control and read alignment of long-read sequencing data Do a differential isoform expression analysis or a repeat expansion analysis Learning experiences This course will consist of lectures, exercises and polls. During exercises, you are free to discuss with other participants. During lectures, focus on the lecture only. Exercises Each block has practical work involved. Some more than others. The practicals are subdivided into chapters, and we\u2019ll have a (short) discussion after each chapter. All answers to the practicals are incorporated, but they are hidden. Do the exercise first by yourself, before checking out the answer. If your answer is different from the answer in the practicals, try to figure out why they are different. Asking questions During lectures, you are encouraged to raise your hand if you have questions (if in-person), or use the Zoom functionality (if online): A main source of communication will be our slack channel . Ask background questions that interest you personally at #background . During the exercises, e.g. if you are stuck or don\u2019t understand what is going on, use the slack channel #q-and-a . This channel is not only meant for asking questions but also for answering questions of other participants. Answer questions only if you have finished the practical block, and use the \u201creply in thread\u201d option: The teacher will review the answers, and add/modify if necessary. If you\u2019re stuck and need tutor support, use the no button in Zoom, if you\u2019re finished use the yes button. To summarize: During lectures: raise hand/zoom functionality Personal interest questions: #background During exercises: #q-and-a on slack if really stuck: no button in zoom if finished: yes button in zoom","title":"Home"},{"location":"#course-website","text":"","title":"Course website"},{"location":"#material","text":"This website Zoom meeting (through mail) Google doc (through mail) Slack channel","title":"Material"},{"location":"#learning-outcomes","text":"After this course, you will be able to: Understand the basics behind PacBio SMRT sequencing and Oxford Nanopore Technology sequencing Use the command line to perform quality control and read alignment of long-read sequencing data Do a differential isoform expression analysis or a repeat expansion analysis","title":"Learning outcomes"},{"location":"#learning-experiences","text":"This course will consist of lectures, exercises and polls. During exercises, you are free to discuss with other participants. During lectures, focus on the lecture only.","title":"Learning experiences"},{"location":"#exercises","text":"Each block has practical work involved. Some more than others. The practicals are subdivided into chapters, and we\u2019ll have a (short) discussion after each chapter. All answers to the practicals are incorporated, but they are hidden. Do the exercise first by yourself, before checking out the answer. If your answer is different from the answer in the practicals, try to figure out why they are different.","title":"Exercises"},{"location":"#asking-questions","text":"During lectures, you are encouraged to raise your hand if you have questions (if in-person), or use the Zoom functionality (if online): A main source of communication will be our slack channel . Ask background questions that interest you personally at #background . During the exercises, e.g. if you are stuck or don\u2019t understand what is going on, use the slack channel #q-and-a . This channel is not only meant for asking questions but also for answering questions of other participants. Answer questions only if you have finished the practical block, and use the \u201creply in thread\u201d option: The teacher will review the answers, and add/modify if necessary. If you\u2019re stuck and need tutor support, use the no button in Zoom, if you\u2019re finished use the yes button. To summarize: During lectures: raise hand/zoom functionality Personal interest questions: #background During exercises: #q-and-a on slack if really stuck: no button in zoom if finished: yes button in zoom","title":"Asking questions"},{"location":"course_schedule/","text":"Day 1 block start end subject block 1 9:00 AM 10:30 AM Introduction 10:30 AM 11:00 AM BREAK block 2 11:00 AM 12:30 PM Server login + unix fresh up 12:30 PM 1:30 PM BREAK block 3 1:30 PM 3:00 PM Quality control and Read alignment 3:00 PM 3:30 PM BREAK block 4 3:30 PM 5:00 PM Applications Day 2 block start end subject block 1 9:00 AM 10:30 AM Group work 10:30 AM 11:00 AM BREAK block 2 11:00 AM 12:30 PM Group work 12:30 PM 1:30 PM BREAK block 3 1:30 PM 3:00 PM Group work 3:00 PM 3:30 PM BREAK block 4 3:30 PM 5:00 PM Presentations","title":"Course schedule"},{"location":"course_schedule/#day-1","text":"block start end subject block 1 9:00 AM 10:30 AM Introduction 10:30 AM 11:00 AM BREAK block 2 11:00 AM 12:30 PM Server login + unix fresh up 12:30 PM 1:30 PM BREAK block 3 1:30 PM 3:00 PM Quality control and Read alignment 3:00 PM 3:30 PM BREAK block 4 3:30 PM 5:00 PM Applications","title":"Day 1"},{"location":"course_schedule/#day-2","text":"block start end subject block 1 9:00 AM 10:30 AM Group work 10:30 AM 11:00 AM BREAK block 2 11:00 AM 12:30 PM Group work 12:30 PM 1:30 PM BREAK block 3 1:30 PM 3:00 PM Group work 3:00 PM 3:30 PM BREAK block 4 3:30 PM 5:00 PM Presentations","title":"Day 2"},{"location":"precourse/","text":"UNIX As is stated in the course prerequisites at the announcement web page . We expect participants to have a basic understanding of working with the command line on UNIX-based systems. You can test your UNIX skills with a quiz here . If you don\u2019t have experience with UNIX command line, or if you\u2019re unsure whether you meet the prerequisites, follow our online UNIX tutorial . Software We will be mainly working on an Amazon Web Services ( AWS ) Elastic Cloud (EC2) server. Our Ubuntu server behaves like a \u2018normal\u2019 remote server, and can be approached through ssh with a username, key and IP address. All participants will be granted access to a personal home directory. Before the course, make sure you can comfortably work on a remote server. This means that you can approach it through the shell, modify scripts and transfer files. We can recommend atom for Linux and Mac, and Notepad ++ in combination with MobaXterm for Windows. We will be visualising our results with IGV. Therefore, install in your computer: mac OS/Linux SSH and scripting: Atom with packages like: terminus and ftp-remote-edit Transferring files: FileZilla Integrative Genomics Viewer (IGV) Windows SSH and scripting: MobaXterm and/or Notepad++ with the plugin NppFTP Transferring files: FileZilla Integrative Genomics Viewer (IGV)","title":"Precourse preparations"},{"location":"precourse/#unix","text":"As is stated in the course prerequisites at the announcement web page . We expect participants to have a basic understanding of working with the command line on UNIX-based systems. You can test your UNIX skills with a quiz here . If you don\u2019t have experience with UNIX command line, or if you\u2019re unsure whether you meet the prerequisites, follow our online UNIX tutorial .","title":"UNIX"},{"location":"precourse/#software","text":"We will be mainly working on an Amazon Web Services ( AWS ) Elastic Cloud (EC2) server. Our Ubuntu server behaves like a \u2018normal\u2019 remote server, and can be approached through ssh with a username, key and IP address. All participants will be granted access to a personal home directory. Before the course, make sure you can comfortably work on a remote server. This means that you can approach it through the shell, modify scripts and transfer files. We can recommend atom for Linux and Mac, and Notepad ++ in combination with MobaXterm for Windows. We will be visualising our results with IGV. Therefore, install in your computer: mac OS/Linux SSH and scripting: Atom with packages like: terminus and ftp-remote-edit Transferring files: FileZilla Integrative Genomics Viewer (IGV) Windows SSH and scripting: MobaXterm and/or Notepad++ with the plugin NppFTP Transferring files: FileZilla Integrative Genomics Viewer (IGV)","title":"Software"},{"location":"course_material/applications/","text":"Learning outcomes After having completed this chapter you will be able to: Explain for what kind of questions long-read sequencing technologies are more suitable compared to short-read sequencing technologies. Describe the basic steps required for a genome assembly Material Slides part 1: Download the presentation Slides part 2 (Heidi Tschanz-Lischer): Download the presentation More on adaptive sampling More on Cas9 targeted sequencing ONT More on Cas9 targeted sequencing PacBio (noAmp)","title":"Applications"},{"location":"course_material/applications/#learning-outcomes","text":"After having completed this chapter you will be able to: Explain for what kind of questions long-read sequencing technologies are more suitable compared to short-read sequencing technologies. Describe the basic steps required for a genome assembly","title":"Learning outcomes"},{"location":"course_material/applications/#material","text":"Slides part 1: Download the presentation Slides part 2 (Heidi Tschanz-Lischer): Download the presentation More on adaptive sampling More on Cas9 targeted sequencing ONT More on Cas9 targeted sequencing PacBio (noAmp)","title":"Material"},{"location":"course_material/group_work/","text":"Learning outcomes After having completed this chapter you will be able to: Develop a basic pipeline for reference-based analysis of a long-read sequencing dataset Answer biological questions based on the results of such an analysis Introduction The last part of this course will consist of project-based-learning. This means that you will work in groups on a single question. We will split up into groups of five people. If working independently If you are working independently, you probably can not work in a group. However, you can test your skills with these real biological datasets. Realize that the datasets and calculations are (much) bigger compared to the exercises, so check if your computer is up for it. You\u2019ll probably need around 4 cores, 16G of RAM and 10G of harddisk. If online If the course takes place online, we will use break-out rooms to communicate within groups. Please stay in the break-out room during the day, also if you are working individually. Roles & organisation Project based learning is about learning by doing, but also about peer instruction . This means that you will be both a learner and a teacher. There will be differences in levels among participants, but because of that, some will learn efficiently from people that have just learned, and others will teach and increase their understanding. Each project has tasks and questions . By performing the tasks, you should be able to answer the questions. At the start of the project, make sure that each of you gets a task assigned. You should consider the tasks and questions as a guidance. If interesting questions pop up during the project, you are encouraged to work on those. Also, you don\u2019t have to perform all the tasks and answer all the questions. In the afternoon of day 1, you will divide the initial tasks, and start on the project. On day 2, you can work on the project in the morning and in the first part of the afternoon. We will conclude the projects with a 10-minute presentation of each group. Project 1: Differential isoform expression analysis of ONT data In this project, you will be working with data from the same resource as the data we have already worked on: Clark, M. B. et al (2020). Long-read sequencing reveals the complex splicing profile of the psychiatric risk gene CACNA1C in human brain . Molecular Psychiatry, 25(1), 37\u201347. https://doi.org/10.1038/s41380-019-0583-1 . It is Oxford Nanopore Technology sequencing data of PCR amplicons of the gene CACNA1C. It is primarily used to discover new splice variants. We will use the dataset to do that and in addition do a differential isoform expression analysis with FLAIR . You can download the required data like this: wget https://ngs-longreads-training.s3.eu-central-1.amazonaws.com/groupwork_ont.tar.gz tar -xvf groupwork_ont.tar.gz rm groupwork_ont.tar.gz This will create a directory groupwork_ont with the following structure: groupwork_ont \u251c\u2500\u2500 alignments \u2502 \u251c\u2500\u2500 cerebellum-5238-batch2.bam \u2502 \u251c\u2500\u2500 cerebellum-5298-batch2.bam \u2502 \u251c\u2500\u2500 cerebellum-5346-batch2.bam \u2502 \u251c\u2500\u2500 parietal_cortex-5238-batch1.bam \u2502 \u251c\u2500\u2500 parietal_cortex-5298-batch1.bam \u2502 \u2514\u2500\u2500 parietal_cortex-5346-batch1.bam \u251c\u2500\u2500 counts \u2502 \u2514\u2500\u2500 counts_matrix_test.tsv \u251c\u2500\u2500 reads \u2502 \u251c\u2500\u2500 cerebellum-5238-batch2.fastq.gz \u2502 \u251c\u2500\u2500 cerebellum-5298-batch2.fastq.gz \u2502 \u251c\u2500\u2500 cerebellum-5346-batch2.fastq.gz \u2502 \u251c\u2500\u2500 parietal_cortex-5238-batch1.fastq.gz \u2502 \u251c\u2500\u2500 parietal_cortex-5298-batch1.fastq.gz \u2502 \u251c\u2500\u2500 parietal_cortex-5346-batch1.fastq.gz \u2502 \u251c\u2500\u2500 striatum-5238-batch2.fastq.gz \u2502 \u251c\u2500\u2500 striatum-5298-batch2.fastq.gz \u2502 \u2514\u2500\u2500 striatum-5346-batch2.fastq.gz \u2514\u2500\u2500 scripts \u2514\u2500\u2500 differential_expression_example.Rmd 4 directories, 17 files Download the fasta file and gtf like this: cd groupwork_ont/ mkdir reference wget ftp://ftp.ensembl.org/pub/release-102/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.chromosome.12.fa.gz wget ftp://ftp.ensembl.org/pub/release-102/gtf/homo_sapiens/Homo_sapiens.GRCh38.102.gtf.gz gunzip *.gz Before you start You can start this project with dividing initial tasks. Because some intermediate files are already given, participants can develop scripts/analyses at different steps of the full analysis from the start. Possible starting points are: Quality control, running fastqc and NanoPlot Alignment, running minimap2 Develop scripts required to run FLAIR Differential expression analysis. Tasks & questions Perform QC with fastqc and with NanoPlot . Do you see a difference between them? How is the read quality compared to the publication? Align each sample separately with minimap2 with default parameters. Set parameters -x and -G to the values we have used during the QC and alignment exercises . You can use 4 threads (set the number of threads with -t ) Start the alignment on day 1 The alignment takes about 6 minutes per sample, so in total about one hour to run. Try to start the alignment at day 1, so you don\u2019t have to wait for the results on day 2. Use nohup myscript.sh & to be able to logout while myscript.sh is running ( tmux and screen are also available). Clone the FLAIR repository to the server, and check out the documentation. Merge the separate alignments with samtools merge , index the merged bam file, and generate a bed12 file with the script flair/bin/bam2Bed12.py Run flair.py correct on the bed12 file. Add the gtf to the options to improve the alignments. Run flair.py collapse to generate isoforms from corrected reads. This steps takes ~1 hour to run. Generate a count matrix with flair.py quantify by using the isoforms fasta and reads_manifest.tsv . Paths in reads_manifest.tsv The paths in reads_manifest.tsv are relative, e.g. reads/striatum-5238-batch2.fastq.gz points to a file relative to the directory from which you are running flair.py quantify . So the directory from which you are running the command should contain the directory reads . If not, modify the paths in the file accordingly (use full paths if you are not sure). Now you can do several things: Do a differential expression analysis. In scripts/ there\u2019s a basic R script to do the analysis. Go to [SERVERIP]:8787 to login to RStudio server. Investigate the isoform usage with the flair script plot_isoform_usage.py Investigate productivity of the different isoforms. Project 2: Repeat expansion analysis of PacBio data You will be working with data from an experiment in which DNA of 8 individuals was sequenced for five different targets by using Pacbio\u2019s no-Amp targeted sequencing system. Two of these targets contain repeat expansions that are related to a disease phenotype. individual disease1 disease2 1015 disease healthy 1016 disease healthy 1017 disease healthy 1018 disease healthy 1019 healthy healthy 1020 healthy disease 1021 healthy disease 1022 healthy disease You can get the reads and sequence targets with: wget https://ngs-longreads-training.s3.eu-central-1.amazonaws.com/groupwork_pacbio.tar.gz tar -xvf groupwork_pacbio.tar.gz rm groupwork_pacbio.tar.gz It has the following directory structure: groupwork_pacbio \u251c\u2500\u2500 alignments \u2502 \u251c\u2500\u2500 bc1020.aln.bam \u2502 \u251c\u2500\u2500 bc1021.aln.bam \u2502 \u2514\u2500\u2500 bc1022.aln.bam \u251c\u2500\u2500 reads \u2502 \u251c\u2500\u2500 1015.fastq.gz \u2502 \u251c\u2500\u2500 1016.fastq.gz \u2502 \u251c\u2500\u2500 1017.fastq.gz \u2502 \u251c\u2500\u2500 1018.fastq.gz \u2502 \u251c\u2500\u2500 1019.fastq.gz \u2502 \u251c\u2500\u2500 1020.fastq.gz \u2502 \u251c\u2500\u2500 1021.fastq.gz \u2502 \u2514\u2500\u2500 1022.fastq.gz \u2514\u2500\u2500 targets \u251c\u2500\u2500 target_gene1_hg38.bed \u2514\u2500\u2500 target_gene2_hg38.bed 3 directories, 13 files The targets in gene1 and gene2 are described in targets/target_gene1_hg38.bed and targets/target_gene2_hg38.bed respectively. The columns in these .bed files describe the chromosome, start, end, name, motifs, and whether the motifs are in reverse complement. You can download the reference genome like this: cd groupwork_pacbio mkdir reference wget ftp://ftp.ensembl.org/pub/release-101/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz gunzip Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz Before you start You can start this project with dividing initial tasks. Because some intermediate files are already given, participants can develop scripts/analyses at different steps of the full analysis from the start. Possible starting points are: Browse IGV to find the genes Perform the QC with NanoPlot Perform the alignment with minimap2 Do the repeat analysis with makeReports.sh Alignment files to do an initial repeat analysis are in the tar.gz package. However, it contains only the files for individuals with disease2. You can develop scripts and analyses based on that. To do the full analysis, all the alignments will need to be run. Tasks & questions Load the bed files into IGV and navigate to the regions they annotate. In which genes are the targets? What kind of diseases are associated with these genes? Perform a quality control with NanoPlot . How is the read quality? These are circular concensus sequences (ccs). Is this quality expected? How is the read length? Align the reads to hg38 with minimap2 . For the option -x you can use asm20 . Generate separate alignment files for each individual. Alternatively use pbmm2 Pacific Biosciences has developed a wrapper for minimap2 that contains settings specific for PacBio reads, named pbmm2 . It might slightly improve your alignments. It is installed in the conda environment. Feel free to give it a try if you have time left. Clone the PacBio apps-scripts repository to the server. The script apps-scripts/RepeatAnalysisTools/makeReports.sh generates repeat expansion reports. Check out the documentation , and generate repeat expansion reports for all individuals on both gene1 and gene2. Check out the report output and read the further documentation of RepeatAnalysisTools . How is the enrichment? Does the clustering make sense? How does the clustering look in IGV? Which individual is affected with which disease? Based on the size of the expansions, can you say something about expected disease severity? This tutorial is based on data provided by Pacific Biosciences at https://downloads.pacbcloud.com/public/dataset/RepeatExpansionDisorders_NoAmp/","title":"Group work"},{"location":"course_material/group_work/#learning-outcomes","text":"After having completed this chapter you will be able to: Develop a basic pipeline for reference-based analysis of a long-read sequencing dataset Answer biological questions based on the results of such an analysis","title":"Learning outcomes"},{"location":"course_material/group_work/#introduction","text":"The last part of this course will consist of project-based-learning. This means that you will work in groups on a single question. We will split up into groups of five people. If working independently If you are working independently, you probably can not work in a group. However, you can test your skills with these real biological datasets. Realize that the datasets and calculations are (much) bigger compared to the exercises, so check if your computer is up for it. You\u2019ll probably need around 4 cores, 16G of RAM and 10G of harddisk. If online If the course takes place online, we will use break-out rooms to communicate within groups. Please stay in the break-out room during the day, also if you are working individually.","title":"Introduction"},{"location":"course_material/group_work/#roles-organisation","text":"Project based learning is about learning by doing, but also about peer instruction . This means that you will be both a learner and a teacher. There will be differences in levels among participants, but because of that, some will learn efficiently from people that have just learned, and others will teach and increase their understanding. Each project has tasks and questions . By performing the tasks, you should be able to answer the questions. At the start of the project, make sure that each of you gets a task assigned. You should consider the tasks and questions as a guidance. If interesting questions pop up during the project, you are encouraged to work on those. Also, you don\u2019t have to perform all the tasks and answer all the questions. In the afternoon of day 1, you will divide the initial tasks, and start on the project. On day 2, you can work on the project in the morning and in the first part of the afternoon. We will conclude the projects with a 10-minute presentation of each group.","title":"Roles &amp; organisation"},{"location":"course_material/group_work/#project-1-differential-isoform-expression-analysis-of-ont-data","text":"In this project, you will be working with data from the same resource as the data we have already worked on: Clark, M. B. et al (2020). Long-read sequencing reveals the complex splicing profile of the psychiatric risk gene CACNA1C in human brain . Molecular Psychiatry, 25(1), 37\u201347. https://doi.org/10.1038/s41380-019-0583-1 . It is Oxford Nanopore Technology sequencing data of PCR amplicons of the gene CACNA1C. It is primarily used to discover new splice variants. We will use the dataset to do that and in addition do a differential isoform expression analysis with FLAIR . You can download the required data like this: wget https://ngs-longreads-training.s3.eu-central-1.amazonaws.com/groupwork_ont.tar.gz tar -xvf groupwork_ont.tar.gz rm groupwork_ont.tar.gz This will create a directory groupwork_ont with the following structure: groupwork_ont \u251c\u2500\u2500 alignments \u2502 \u251c\u2500\u2500 cerebellum-5238-batch2.bam \u2502 \u251c\u2500\u2500 cerebellum-5298-batch2.bam \u2502 \u251c\u2500\u2500 cerebellum-5346-batch2.bam \u2502 \u251c\u2500\u2500 parietal_cortex-5238-batch1.bam \u2502 \u251c\u2500\u2500 parietal_cortex-5298-batch1.bam \u2502 \u2514\u2500\u2500 parietal_cortex-5346-batch1.bam \u251c\u2500\u2500 counts \u2502 \u2514\u2500\u2500 counts_matrix_test.tsv \u251c\u2500\u2500 reads \u2502 \u251c\u2500\u2500 cerebellum-5238-batch2.fastq.gz \u2502 \u251c\u2500\u2500 cerebellum-5298-batch2.fastq.gz \u2502 \u251c\u2500\u2500 cerebellum-5346-batch2.fastq.gz \u2502 \u251c\u2500\u2500 parietal_cortex-5238-batch1.fastq.gz \u2502 \u251c\u2500\u2500 parietal_cortex-5298-batch1.fastq.gz \u2502 \u251c\u2500\u2500 parietal_cortex-5346-batch1.fastq.gz \u2502 \u251c\u2500\u2500 striatum-5238-batch2.fastq.gz \u2502 \u251c\u2500\u2500 striatum-5298-batch2.fastq.gz \u2502 \u2514\u2500\u2500 striatum-5346-batch2.fastq.gz \u2514\u2500\u2500 scripts \u2514\u2500\u2500 differential_expression_example.Rmd 4 directories, 17 files Download the fasta file and gtf like this: cd groupwork_ont/ mkdir reference wget ftp://ftp.ensembl.org/pub/release-102/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.chromosome.12.fa.gz wget ftp://ftp.ensembl.org/pub/release-102/gtf/homo_sapiens/Homo_sapiens.GRCh38.102.gtf.gz gunzip *.gz","title":" Project 1: Differential isoform expression analysis of ONT data"},{"location":"course_material/group_work/#before-you-start","text":"You can start this project with dividing initial tasks. Because some intermediate files are already given, participants can develop scripts/analyses at different steps of the full analysis from the start. Possible starting points are: Quality control, running fastqc and NanoPlot Alignment, running minimap2 Develop scripts required to run FLAIR Differential expression analysis.","title":"Before you start"},{"location":"course_material/group_work/#tasks-questions","text":"Perform QC with fastqc and with NanoPlot . Do you see a difference between them? How is the read quality compared to the publication? Align each sample separately with minimap2 with default parameters. Set parameters -x and -G to the values we have used during the QC and alignment exercises . You can use 4 threads (set the number of threads with -t ) Start the alignment on day 1 The alignment takes about 6 minutes per sample, so in total about one hour to run. Try to start the alignment at day 1, so you don\u2019t have to wait for the results on day 2. Use nohup myscript.sh & to be able to logout while myscript.sh is running ( tmux and screen are also available). Clone the FLAIR repository to the server, and check out the documentation. Merge the separate alignments with samtools merge , index the merged bam file, and generate a bed12 file with the script flair/bin/bam2Bed12.py Run flair.py correct on the bed12 file. Add the gtf to the options to improve the alignments. Run flair.py collapse to generate isoforms from corrected reads. This steps takes ~1 hour to run. Generate a count matrix with flair.py quantify by using the isoforms fasta and reads_manifest.tsv . Paths in reads_manifest.tsv The paths in reads_manifest.tsv are relative, e.g. reads/striatum-5238-batch2.fastq.gz points to a file relative to the directory from which you are running flair.py quantify . So the directory from which you are running the command should contain the directory reads . If not, modify the paths in the file accordingly (use full paths if you are not sure). Now you can do several things: Do a differential expression analysis. In scripts/ there\u2019s a basic R script to do the analysis. Go to [SERVERIP]:8787 to login to RStudio server. Investigate the isoform usage with the flair script plot_isoform_usage.py Investigate productivity of the different isoforms.","title":"Tasks &amp; questions"},{"location":"course_material/group_work/#project-2-repeat-expansion-analysis-of-pacbio-data","text":"You will be working with data from an experiment in which DNA of 8 individuals was sequenced for five different targets by using Pacbio\u2019s no-Amp targeted sequencing system. Two of these targets contain repeat expansions that are related to a disease phenotype. individual disease1 disease2 1015 disease healthy 1016 disease healthy 1017 disease healthy 1018 disease healthy 1019 healthy healthy 1020 healthy disease 1021 healthy disease 1022 healthy disease You can get the reads and sequence targets with: wget https://ngs-longreads-training.s3.eu-central-1.amazonaws.com/groupwork_pacbio.tar.gz tar -xvf groupwork_pacbio.tar.gz rm groupwork_pacbio.tar.gz It has the following directory structure: groupwork_pacbio \u251c\u2500\u2500 alignments \u2502 \u251c\u2500\u2500 bc1020.aln.bam \u2502 \u251c\u2500\u2500 bc1021.aln.bam \u2502 \u2514\u2500\u2500 bc1022.aln.bam \u251c\u2500\u2500 reads \u2502 \u251c\u2500\u2500 1015.fastq.gz \u2502 \u251c\u2500\u2500 1016.fastq.gz \u2502 \u251c\u2500\u2500 1017.fastq.gz \u2502 \u251c\u2500\u2500 1018.fastq.gz \u2502 \u251c\u2500\u2500 1019.fastq.gz \u2502 \u251c\u2500\u2500 1020.fastq.gz \u2502 \u251c\u2500\u2500 1021.fastq.gz \u2502 \u2514\u2500\u2500 1022.fastq.gz \u2514\u2500\u2500 targets \u251c\u2500\u2500 target_gene1_hg38.bed \u2514\u2500\u2500 target_gene2_hg38.bed 3 directories, 13 files The targets in gene1 and gene2 are described in targets/target_gene1_hg38.bed and targets/target_gene2_hg38.bed respectively. The columns in these .bed files describe the chromosome, start, end, name, motifs, and whether the motifs are in reverse complement. You can download the reference genome like this: cd groupwork_pacbio mkdir reference wget ftp://ftp.ensembl.org/pub/release-101/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz gunzip Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz","title":" Project 2: Repeat expansion analysis of PacBio data"},{"location":"course_material/group_work/#before-you-start_1","text":"You can start this project with dividing initial tasks. Because some intermediate files are already given, participants can develop scripts/analyses at different steps of the full analysis from the start. Possible starting points are: Browse IGV to find the genes Perform the QC with NanoPlot Perform the alignment with minimap2 Do the repeat analysis with makeReports.sh Alignment files to do an initial repeat analysis are in the tar.gz package. However, it contains only the files for individuals with disease2. You can develop scripts and analyses based on that. To do the full analysis, all the alignments will need to be run.","title":"Before you start"},{"location":"course_material/group_work/#tasks-questions_1","text":"Load the bed files into IGV and navigate to the regions they annotate. In which genes are the targets? What kind of diseases are associated with these genes? Perform a quality control with NanoPlot . How is the read quality? These are circular concensus sequences (ccs). Is this quality expected? How is the read length? Align the reads to hg38 with minimap2 . For the option -x you can use asm20 . Generate separate alignment files for each individual. Alternatively use pbmm2 Pacific Biosciences has developed a wrapper for minimap2 that contains settings specific for PacBio reads, named pbmm2 . It might slightly improve your alignments. It is installed in the conda environment. Feel free to give it a try if you have time left. Clone the PacBio apps-scripts repository to the server. The script apps-scripts/RepeatAnalysisTools/makeReports.sh generates repeat expansion reports. Check out the documentation , and generate repeat expansion reports for all individuals on both gene1 and gene2. Check out the report output and read the further documentation of RepeatAnalysisTools . How is the enrichment? Does the clustering make sense? How does the clustering look in IGV? Which individual is affected with which disease? Based on the size of the expansions, can you say something about expected disease severity? This tutorial is based on data provided by Pacific Biosciences at https://downloads.pacbcloud.com/public/dataset/RepeatExpansionDisorders_NoAmp/","title":"Tasks &amp; questions"},{"location":"course_material/introduction/","text":"Learning outcomes After having completed this chapter you will be able to: Define the difference between short-read and long-read sequencing Explain which type of invention led to development of long-read sequencing Describe the basic techniques behind Oxford Nanopore sequencing and PacBio sequencing Decide based on the characteristics of the discussed sequencing platforms which one is most suited for different situations Material The introduction presentation: Download the presentation The sequencing technologies presentation: Download the presentation","title":"Introduction"},{"location":"course_material/introduction/#learning-outcomes","text":"After having completed this chapter you will be able to: Define the difference between short-read and long-read sequencing Explain which type of invention led to development of long-read sequencing Describe the basic techniques behind Oxford Nanopore sequencing and PacBio sequencing Decide based on the characteristics of the discussed sequencing platforms which one is most suited for different situations","title":"Learning outcomes"},{"location":"course_material/introduction/#material","text":"","title":"Material"},{"location":"course_material/introduction/#the-introduction-presentation","text":"Download the presentation","title":"The introduction presentation:"},{"location":"course_material/introduction/#the-sequencing-technologies-presentation","text":"Download the presentation","title":"The sequencing technologies presentation:"},{"location":"course_material/qc_alignment/","text":"Learning outcomes After having completed this chapter you will be able to: Define how the fastq format stores sequence and base quality information Calculate accuracy and probability based on base quality Describe how alignment information is stored in a sequence alignment ( .sam ) file Explain why the fastq format is limited for long-read sequencing data Perform a quality control on long-read data with NanoPlot Interpret the quality report produced by NanoPlot Perform a basic alignment of long reads with minimap2 , and select arguments to the options -x and -G Visualise an alignment file in IGV on a local computer Material Download the presentation Exercises 1. Retrieve data We will be working with data from: Clark, M. B. et al (2020). Long-read sequencing reveals the complex splicing profile of the psychiatric risk gene CACNA1C in human brain . Molecular Psychiatry, 25(1), 37\u201347. https://doi.org/10.1038/s41380-019-0583-1 The authors used full-transcript amplicon sequencing with Oxford Nanopore Technology of CACNA1C, a gene associated with psychiatric risk. For the exercises of today, we will work with a single sample of this study. Download and unpack the data files in your working directory ( ~/workdir ). cd ~/workdir wget https://ngs-longreads-training.s3.eu-central-1.amazonaws.com/ngs-longreads-training.tar.gz tar -xvf ngs-longreads-training.tar.gz rm ngs-longreads-training.tar.gz Exercise: This will create the directory data . Check out what\u2019s in there. Answer The data folder contains the following: data/ \u251c\u2500\u2500 reads \u2502 \u2514\u2500\u2500 cerebellum-5238-batch2.fastq.gz \u2514\u2500\u2500 reference \u251c\u2500\u2500 Homo_sapiens.GRCh38.102.gtf \u251c\u2500\u2500 Homo_sapiens.GRCh38.dna.chromosome.12.fa \u2514\u2500\u2500 Homo_sapiens.GRCh38.dna.chromosome.12.fa.fai 2 directories, 4 files In the reads folder a fastq file with reads, in the reference folder, a gtf with annotations, the reference sequence and an index of the reference sequence. 2. Quality control We will evaluate the read quality with NanoPlot . Exercise: Check out the manual of NanoPlot with the command NanoPlot --help , and run NanoPlot on data/reads/cerebellum-5238-batch2.fastq.gz . Hint For a basic output of NanoPlot on a fastq.gz file you can use the options --outdir and --fastq Answer We have a fastq file, so based on the manual and the example we can run: NanoPlot \\ --fastq data/reads/cerebellum-5238-batch2.fastq.gz \\ --outdir nanoplot_output You will now have a directory with the following files: \u251c\u2500\u2500 Dynamic_Histogram_Read_length.html \u251c\u2500\u2500 Dynamic_Histogram_Read_length.png \u251c\u2500\u2500 HistogramReadlength.png \u251c\u2500\u2500 LengthvsQualityScatterPlot_dot.png \u251c\u2500\u2500 LengthvsQualityScatterPlot_kde.png \u251c\u2500\u2500 LogTransformed_HistogramReadlength.png \u251c\u2500\u2500 NanoPlot_20210105_1339.log \u251c\u2500\u2500 NanoPlot-report.html \u251c\u2500\u2500 NanoStats.txt \u251c\u2500\u2500 Weighted_HistogramReadlength.png \u251c\u2500\u2500 Weighted_LogTransformed_HistogramReadlength.png \u2514\u2500\u2500 Yield_By_Length.png The file NanoPlot-report.html contains a report with all the information stored in the other files. Exercise: Download NanoPlot-report.html to your local computer and answer the following questions: A. How many reads are in the file? B. What is the average read length? Is there a wide distribution? Given that these sequences are generated from a long-range PCR, is that expected? C. What is the average base quality and what kind of accuracy do we therefore expect? Answer A. 3735 B. The average read length is 6,003.3 base pairs. From the read length histogram we can see that there is a very narrow distribution. As a PCR will generate sequences of approximately the same length, this is expected. C. The average base quality is 7.3. We have learned that \\(p=10^{\\frac{-baseQ}{10}}\\) , so the average probability that the base is wrong is \\(10^{\\frac{-7.3}{10}} = 0.186\\) . The expected accuracy is \\(1-0.186=0.814\\) or 81.4%. 3. Read alignment The sequence aligner minimap2 is specifically developed for (splice-aware) alignment of long reads. Exercise: Checkout the helper minimap2 --help and/or the github readme . We are working with reads generated from cDNA. Considering we are aligning to a reference genome (DNA), what would be the most logical parameter for our dataset to the option -x ? Answer The option -x can take the following arguments: -x STR preset (always applied before other options; see minimap2.1 for details) [] - map-pb/map-ont: PacBio/Nanopore vs reference mapping - ava-pb/ava-ont: PacBio/Nanopore read overlap - asm5/asm10/asm20: asm-to-ref mapping, for ~0.1/1/5% sequence divergence - splice: long-read spliced alignment - sr: genomic short-read mapping We are working with ONT data so we could choose map-ont . However, our data is also spliced. Therefore, we should choose splice . Introns can be quite long in mammals; up to a few hundred kb. Exercise: Look up the CACNA1C gene in hg38 in IGV, and roughly estimate the length of the longest intron. Hint First load hg38 in IGV, by clicking the topleft drop-down menu: After that type CACNA1C in the search box: Answer The longest intron is about 350 kilo bases (350,000 base pairs) Exercise: Check out the -G option of minimap2 . How does this relate to the the largest intron size of CACNA1C? Answer This is what the manual says: -G NUM max intron length (effective with -xsplice; changing -r) [200k] We found an intron size of approximately 350k, so the default is set too small. We should be increase it to at least 350k. Exercise: Make a directory called alignments in your working directory. After that, modify the command below for minimap2 and run it from a script. #!/usr/bin/env bash minimap2 \\ -a \\ -x [ PARAMETER ] \\ -G [ PARAMETER ] \\ -t 2 \\ data/references/GRCh38.p13.chr12.fa \\ data/reads/cerebellum-5238-batch2.fastq.gz \\ | samtools sort \\ | samtools view -bh > alignments/cerebellum-5238-batch2.bam ## indexing for IGV samtools index alignments/cerebellum-5238-batch2.bam Answer Make a directory like this: mkdir ~/workdir/alignments Modify the script to set the -x and -G options: #!/usr/bin/env bash minimap2 \\ -a \\ -x splice \\ -G 500k \\ -t 2 \\ data/reference/Homo_sapiens.GRCh38.dna.chromosome.12.fa \\ data/reads/cerebellum-5238-batch2.fastq.gz \\ | samtools sort \\ | samtools view -bh > alignments/cerebellum-5238-batch2.bam ## indexing for IGV samtools index alignments/cerebellum-5238-batch2.bam And run it (e.g. if you named the script ont_alignment.sh ): chmod u+x ont_alignment.sh ./ont_alignment.sh 4. Visualisation Let\u2019s have a look at the alignments. Download the files cerebellum-5238-batch2.bam and cerebellum-5238-batch2.bam.bai to your local computer and load the .bam file into IGV ( File > Load from File\u2026 ). Exercise: Have a look at the region chr12:2,632,655-2,635,447 by typing it into the search box. Do you see any evidence for alternative splicing already? Answer The two exons seem to be mutually exclusive:","title":"QC and alignment"},{"location":"course_material/qc_alignment/#learning-outcomes","text":"After having completed this chapter you will be able to: Define how the fastq format stores sequence and base quality information Calculate accuracy and probability based on base quality Describe how alignment information is stored in a sequence alignment ( .sam ) file Explain why the fastq format is limited for long-read sequencing data Perform a quality control on long-read data with NanoPlot Interpret the quality report produced by NanoPlot Perform a basic alignment of long reads with minimap2 , and select arguments to the options -x and -G Visualise an alignment file in IGV on a local computer","title":"Learning outcomes"},{"location":"course_material/qc_alignment/#material","text":"Download the presentation","title":"Material"},{"location":"course_material/qc_alignment/#exercises","text":"","title":"Exercises"},{"location":"course_material/qc_alignment/#1-retrieve-data","text":"We will be working with data from: Clark, M. B. et al (2020). Long-read sequencing reveals the complex splicing profile of the psychiatric risk gene CACNA1C in human brain . Molecular Psychiatry, 25(1), 37\u201347. https://doi.org/10.1038/s41380-019-0583-1 The authors used full-transcript amplicon sequencing with Oxford Nanopore Technology of CACNA1C, a gene associated with psychiatric risk. For the exercises of today, we will work with a single sample of this study. Download and unpack the data files in your working directory ( ~/workdir ). cd ~/workdir wget https://ngs-longreads-training.s3.eu-central-1.amazonaws.com/ngs-longreads-training.tar.gz tar -xvf ngs-longreads-training.tar.gz rm ngs-longreads-training.tar.gz Exercise: This will create the directory data . Check out what\u2019s in there. Answer The data folder contains the following: data/ \u251c\u2500\u2500 reads \u2502 \u2514\u2500\u2500 cerebellum-5238-batch2.fastq.gz \u2514\u2500\u2500 reference \u251c\u2500\u2500 Homo_sapiens.GRCh38.102.gtf \u251c\u2500\u2500 Homo_sapiens.GRCh38.dna.chromosome.12.fa \u2514\u2500\u2500 Homo_sapiens.GRCh38.dna.chromosome.12.fa.fai 2 directories, 4 files In the reads folder a fastq file with reads, in the reference folder, a gtf with annotations, the reference sequence and an index of the reference sequence.","title":"1. Retrieve data"},{"location":"course_material/qc_alignment/#2-quality-control","text":"We will evaluate the read quality with NanoPlot . Exercise: Check out the manual of NanoPlot with the command NanoPlot --help , and run NanoPlot on data/reads/cerebellum-5238-batch2.fastq.gz . Hint For a basic output of NanoPlot on a fastq.gz file you can use the options --outdir and --fastq Answer We have a fastq file, so based on the manual and the example we can run: NanoPlot \\ --fastq data/reads/cerebellum-5238-batch2.fastq.gz \\ --outdir nanoplot_output You will now have a directory with the following files: \u251c\u2500\u2500 Dynamic_Histogram_Read_length.html \u251c\u2500\u2500 Dynamic_Histogram_Read_length.png \u251c\u2500\u2500 HistogramReadlength.png \u251c\u2500\u2500 LengthvsQualityScatterPlot_dot.png \u251c\u2500\u2500 LengthvsQualityScatterPlot_kde.png \u251c\u2500\u2500 LogTransformed_HistogramReadlength.png \u251c\u2500\u2500 NanoPlot_20210105_1339.log \u251c\u2500\u2500 NanoPlot-report.html \u251c\u2500\u2500 NanoStats.txt \u251c\u2500\u2500 Weighted_HistogramReadlength.png \u251c\u2500\u2500 Weighted_LogTransformed_HistogramReadlength.png \u2514\u2500\u2500 Yield_By_Length.png The file NanoPlot-report.html contains a report with all the information stored in the other files. Exercise: Download NanoPlot-report.html to your local computer and answer the following questions: A. How many reads are in the file? B. What is the average read length? Is there a wide distribution? Given that these sequences are generated from a long-range PCR, is that expected? C. What is the average base quality and what kind of accuracy do we therefore expect? Answer A. 3735 B. The average read length is 6,003.3 base pairs. From the read length histogram we can see that there is a very narrow distribution. As a PCR will generate sequences of approximately the same length, this is expected. C. The average base quality is 7.3. We have learned that \\(p=10^{\\frac{-baseQ}{10}}\\) , so the average probability that the base is wrong is \\(10^{\\frac{-7.3}{10}} = 0.186\\) . The expected accuracy is \\(1-0.186=0.814\\) or 81.4%.","title":"2. Quality control"},{"location":"course_material/qc_alignment/#3-read-alignment","text":"The sequence aligner minimap2 is specifically developed for (splice-aware) alignment of long reads. Exercise: Checkout the helper minimap2 --help and/or the github readme . We are working with reads generated from cDNA. Considering we are aligning to a reference genome (DNA), what would be the most logical parameter for our dataset to the option -x ? Answer The option -x can take the following arguments: -x STR preset (always applied before other options; see minimap2.1 for details) [] - map-pb/map-ont: PacBio/Nanopore vs reference mapping - ava-pb/ava-ont: PacBio/Nanopore read overlap - asm5/asm10/asm20: asm-to-ref mapping, for ~0.1/1/5% sequence divergence - splice: long-read spliced alignment - sr: genomic short-read mapping We are working with ONT data so we could choose map-ont . However, our data is also spliced. Therefore, we should choose splice . Introns can be quite long in mammals; up to a few hundred kb. Exercise: Look up the CACNA1C gene in hg38 in IGV, and roughly estimate the length of the longest intron. Hint First load hg38 in IGV, by clicking the topleft drop-down menu: After that type CACNA1C in the search box: Answer The longest intron is about 350 kilo bases (350,000 base pairs) Exercise: Check out the -G option of minimap2 . How does this relate to the the largest intron size of CACNA1C? Answer This is what the manual says: -G NUM max intron length (effective with -xsplice; changing -r) [200k] We found an intron size of approximately 350k, so the default is set too small. We should be increase it to at least 350k. Exercise: Make a directory called alignments in your working directory. After that, modify the command below for minimap2 and run it from a script. #!/usr/bin/env bash minimap2 \\ -a \\ -x [ PARAMETER ] \\ -G [ PARAMETER ] \\ -t 2 \\ data/references/GRCh38.p13.chr12.fa \\ data/reads/cerebellum-5238-batch2.fastq.gz \\ | samtools sort \\ | samtools view -bh > alignments/cerebellum-5238-batch2.bam ## indexing for IGV samtools index alignments/cerebellum-5238-batch2.bam Answer Make a directory like this: mkdir ~/workdir/alignments Modify the script to set the -x and -G options: #!/usr/bin/env bash minimap2 \\ -a \\ -x splice \\ -G 500k \\ -t 2 \\ data/reference/Homo_sapiens.GRCh38.dna.chromosome.12.fa \\ data/reads/cerebellum-5238-batch2.fastq.gz \\ | samtools sort \\ | samtools view -bh > alignments/cerebellum-5238-batch2.bam ## indexing for IGV samtools index alignments/cerebellum-5238-batch2.bam And run it (e.g. if you named the script ont_alignment.sh ): chmod u+x ont_alignment.sh ./ont_alignment.sh","title":"3. Read alignment"},{"location":"course_material/qc_alignment/#4-visualisation","text":"Let\u2019s have a look at the alignments. Download the files cerebellum-5238-batch2.bam and cerebellum-5238-batch2.bam.bai to your local computer and load the .bam file into IGV ( File > Load from File\u2026 ). Exercise: Have a look at the region chr12:2,632,655-2,635,447 by typing it into the search box. Do you see any evidence for alternative splicing already? Answer The two exons seem to be mutually exclusive:","title":"4. Visualisation"},{"location":"course_material/server_login/","text":"Learning outcomes After having completed this chapter you will be able to: Setup a connection between a local computer and remote host with ssh Setup and use a script editor to add and modify scripts on a remote computer Setup and use FileZilla to copy files from a remote machine to a local computer Use the command line to: Make a directory Change file permissions to \u2018executable\u2019 Run a bash script Pipe data from and to a file or other executable Program a loop in bash Choose your OS or platform In this part we will set up your computer to work on the remote AWS server, or setup your computer to do the exercises with conda or with Docker. If you are doing the course with a teacher , you will have to login to the remote server. Therefore choose either: macOS/Linux Windows If you are doing this course independently (i.e. without a teacher) choose either: conda Docker mac OS/Linux Material You have received an e-mail shortly before the workshop with a key, username and IP address to login on a cloud server. Below you can find the material that helps you to login, edit scripts and transfer files. Great power comes with great responsibility The cloud server is a temporary instance for this workshop only. Although the computational resources should be more than enough, it\u2019s a basic Ubuntu server, and there are no hard limits on memory or CPU usage. Take therefore into account that great power comes with great responsibility. Overloading it can result in a reboot, cancelling all running calculations. Video tutorials Set up ftp-remote-edit for Atom Set up FileZilla Exercises Login to AWS EC2 remote server In this part, we\u2019ll use the video tutorials and the information below to log in and set up a remote script editor. Open a terminal and login like this: ssh -i path/to/key/key_<username>.pem <username>@<IP> Warning change path/to/key to the actual path where you have put the key file. replace and with your actual username and IP Setup your Atom and FileZilla Atom Atom is a versatile text editor for all major operating systems. For this course, it\u2019s the recommended script editor for Linux and Mac OS users. With the third-party package ftp-remote-edit , you can remotely edit scripts. Set it up on your own computer using your own credentials and the video below. In general, setup the connection to the server with the following details: protocol: sftp username: your username hostname: server IP port: 22 authentication/logon type: path to private key file FileZilla Many results come in an image (e.g. .png , .jpg ) or html format. These can not be viewed directly from the server. Also, for this course, files loaded in IGV need to be on your local computer. You can easily transfer files between your local PC and the remote host with FileZilla . Set it up on your own computer using your own credentials and the video below. Initiate conda To make use of the pre-installed software with conda, we need to initiate it first. Login to the server and run: /opt/miniconda3/bin/conda init exec bash Now, your shell should start with (base) , meaning that the conda base environment is loaded. To load the environment longreads with the required software packages, run: conda activate longreads Which should change the start of your shell from (base) to (longreads) Activating the environment You will need to activate the longreads environment each time you login. Windows Material You have received an e-mail shortly before the workshop with a key, username and IP address to login on a cloud server. Below you can find the material that helps you to login, edit scripts and transfer files. Great power comes with great responsibility The cloud server is a temporary instance for this workshop only. Although the computational resources should be more than enough, it\u2019s a basic Ubuntu server, and there are no hard limits on memory or CPU usage. Take therefore into account that great power comes with great responsibility. Overloading it can result in a reboot, cancelling all running calculations. Video tutorials Set up MobaXterm Set up FileZilla Exercises Set up MobaXterm In this part, you will use the video tutorials and the information below to log in and set up a remote script editor. MobaXterm is an SSH client for Windows. Use this to connect to the remote host and edit remote scripts. With MobaXterm, you will automatically login to the remote server once you\u2019ve started the SSH session. Set it up on your own computer using your own credentials and the video below. These are the general settings you should take into account: protocol: sftp username: your username hostname: server IP port: 22 authentication/logon type: path to private key file Set up FileZilla Many results come in an image (e.g. .png , .jpg ) or html format. These can not be viewed directly from the server. Also, for this course, files loaded in IGV need to be on your local computer. You can easily transfer files between your local PC and the remote host with FileZilla . Set it up on your own computer using your own credentials and the video below. Initiate conda To make use of the pre-installed software with conda, we need to initiate it first. Login to the server and run: /opt/miniconda3/bin/conda init exec bash Now, your shell should start with (base) , meaning that the conda base environment is loaded. To load the environment longreads with the required software packages, run: conda activate longreads Which should change the start of your shell from (base) to (longreads) Activating the environment You will need to activate the longreads environment each time you login. Docker Material Instructions to install docker Instructions to set up to container Exercises First login Docker can be used to run an entire isolated environment in a container. This means that we can run the software with all its dependencies required for this course locally in your computer. Independent of your operating system. In the video below there\u2019s a tutorial on how to set up a docker container for this course. Note that you will need administrator rights, and that if you are using Windows, you need the latest version of Windows 10. The command to run the environment required for this course looks like this (in a terminal or powershell): Modify the script Modify the path after -v to the working directory on your computer before running it. Mac OS/Linux terminal docker run \\ -v /full/path/to/local/workdir:/root/workdir \\ -i -t \\ geertvangeest/ngs-longreads \\ /bin/bash Windows powershell docker run ` -v C : \\ Users \\ myusername : / root / workdir ` -i -t ` geertvangeest / ngs-longreads ` / bin / bash The option -v mounts a local directory in your computer to the directory /root/workdir in the docker container. In that way, you have files available both in the container and on your computer. Use this directory on your computer to e.g. edit scripts and visualise data with IGV. Change the first path to a path on your computer that you want to use as a working directory. Don\u2019t mount directly in the home dir Don\u2019t directly mount your local directory to the home directory ( /root ). This will lead to unexpected behaviour. The options -i and -t let you approach the container interactively. Meaning that you can use the shell. The part geertvangeest/ngs-longreads is the image we are going to load into the container. The image contains all the information about software and dependencies needed for this course. When you run this command for the first time it will download the image. Once it\u2019s on your computer, it will start immediately. The last bit /bin/bash tells us which entrypoint we take. Which is the bash command line interpreter. You can exit the shell with exit . Working with a running container Restarting After exiting, you can restart the container. Find the container name: docker container ls -a The name is e.g. adoring_bell . To restart run: docker start adoring_bell docker attach adoring_bell Second shell If you want to have a second shell in your container, e.g. because your current shell is busy, you can use: docker exec -it adoring_bell /bin/bash Difference docker attach and docker exec Difference between the commands is explainer here . Conclusion: do not run docker attach for a second shell in which you usually want to start a new process. Lost the container If you lost the container for whatever reason, no problem. If you did all your work in the mounted workdir, you can just remount it to a new container based on the same image. To do that, just rerun the docker run command (with the option -v , -i , -t and the entrypoint). Save your own version If you have additional installations, and you want to keep them, you can save the image with: docker commit adoring_bell my-image conda If you have a conda installation on your local computer, you can install the required software using conda. You can build the environment from ngs-longreads.yml Generate the conda environment like this: conda env create --name ngs-longreads -f ngs-longreads.yml The yaml file probably only works for Linux systems If you want to use the conda environment on a different OS, use: conda create -n ngs-longreads python = 3 .6 conda activate ngs-longreads conda install -y -c bioconda \\ samtools \\ minimap2 \\ fastqc \\ pbmm2 \\ conda install -y -c bioconda nanoplot If the installation of NanoPlot fails, try to install it with pip : pip install NanoPlot This will create the conda environment ngs-longreads Activate it like so: conda activate ngs-longreads After successful installation and activating the environment all the software required to do the exercises should be available. A UNIX command line interface (CLI) refresher Most bioinformatics software are UNIX based and are executed through the CLI. When working with NGS data, it is therefore convenient to improve your knowledge on UNIX. For this course, we need basic understanding of UNIX CLI, so here are some exercises to refresh your memory. Make a new directory Login to the server and use the command line to make a directory called workdir . If working with Docker If your are working with docker you are a root user. This means that your \u201chome\u201d directory is the root directory, i.e. /root , and not /home/username . If you have mounted your local directory to /root/workdir , this directory should already exist. Answer cd mkdir workdir Make a directory scripts within ~/workdir and make it your current directory. Answer cd workdir mkdir scripts cd scripts File permissions Generate an empty script in your newly made directory ~/workdir/scripts like this: touch new_script.sh Add a command to this script that writes \u201cSIB courses are great!\u201d (or something you can better relate to.. ) to stdout, and try to run it. Answer You can use your remote script editor to edit your script. Otherwise you can use nano to edit it: nano new_script.sh The script should look like this: #!/usr/bin/env bash echo \"SIB courses are great!\" Usually, you can run it like this: ./new_script.sh But there\u2019s an error: bash: ./new_script.sh: Permission denied Why is there an error? Hint Use ls -lh new_script.sh to check the permissions. Answer ls -lh new_script.sh gives: -rw-r--r-- 1 user group 51B Nov 11 16 :21 new_script.sh There\u2019s no x in the permissions string. You should change at least the permissions of the user. Make the script executable for yourself, and run it. Answer Change permissions: chmod u+x new_script.sh ls -lh new_script.sh now gives: -rwxr--r-- 1 user group 51B Nov 11 16:21 new_script.sh So it should be executable: ./new_script.sh More on chmod and file permissions here . Redirection: > and | In the root directory (go there like this: cd / ) there are a range of system directories and files. Write the names of all directories and files to a file called system_dirs.txt in your home directory (use ls and > ). Answer ls / > ~/system_dirs.txt The command wc -l counts the number of lines, and can read from stdin. Make a one-liner with a pipe | symbol to find out how many system directories and files there are. Answer ls / | wc -l Variables Store system_dirs.txt as variable (like this: VAR=variable ), and use wc -l on that variable to count the number of lines in the file. Answer FILE = system_dirs.txt wc -l $FILE shell scripts Make a shell script that automatically counts the number of system directories and files. Answer Make a script called e.g. current_system_dirs.sh : #!/usr/bin/env bash cd / ls | wc -l 3. Loops 20 minutes If you want to run the same command on a range of arguments, it\u2019s not very convenient to type the command for each individual argument. For example, you could write dog , fox , bird to stdout in a script like this: #!/usr/bin/env bash echo dog echo fox echo bird However, if you want to change the command (add an option for example), you would have to change it for all the three command calls. Amongst others for that reason, you want to write the command only once. You can do this with a for-loop, like this: #!/usr/bin/env bash ANIMALS = \"dog fox bird\" for animal in $ANIMALS do echo $animal done Which results in: dog fox bird Write a shell script that removes all the letters \u201ce\u201d from a list of words. Hint Removing the letter \u201ce\u201d from a string can be done with tr like this: word = \"test\" echo $word | tr -d \"e\" Which would result in: tst Answer Your script should e.g. look like this (I\u2019ve added some awesome functionality): #!/usr/bin/env bash WORDLIST = \"here is a list of words resulting in a sentence\" for word in $WORDLIST do echo \"' $word ' with e's removed looks like:\" echo $word | tr -d \"e\" done resulting in: 'here' with e's removed looks like: hr 'is' with e's removed looks like: is 'a' with e's removed looks like: a 'list' with e's removed looks like: list 'of' with e's removed looks like: of 'words' with e's removed looks like: words 'resulting' with e's removed looks like: rsulting 'in' with e's removed looks like: in 'a' with e's removed looks like: a 'sentence' with e's removed looks like: sntnc","title":"Server login"},{"location":"course_material/server_login/#learning-outcomes","text":"After having completed this chapter you will be able to: Setup a connection between a local computer and remote host with ssh Setup and use a script editor to add and modify scripts on a remote computer Setup and use FileZilla to copy files from a remote machine to a local computer Use the command line to: Make a directory Change file permissions to \u2018executable\u2019 Run a bash script Pipe data from and to a file or other executable Program a loop in bash Choose your OS or platform In this part we will set up your computer to work on the remote AWS server, or setup your computer to do the exercises with conda or with Docker. If you are doing the course with a teacher , you will have to login to the remote server. Therefore choose either: macOS/Linux Windows If you are doing this course independently (i.e. without a teacher) choose either: conda Docker mac OS/Linux","title":"Learning outcomes"},{"location":"course_material/server_login/#material","text":"You have received an e-mail shortly before the workshop with a key, username and IP address to login on a cloud server. Below you can find the material that helps you to login, edit scripts and transfer files. Great power comes with great responsibility The cloud server is a temporary instance for this workshop only. Although the computational resources should be more than enough, it\u2019s a basic Ubuntu server, and there are no hard limits on memory or CPU usage. Take therefore into account that great power comes with great responsibility. Overloading it can result in a reboot, cancelling all running calculations.","title":"Material"},{"location":"course_material/server_login/#video-tutorials","text":"Set up ftp-remote-edit for Atom Set up FileZilla","title":"Video tutorials"},{"location":"course_material/server_login/#exercises","text":"","title":"Exercises"},{"location":"course_material/server_login/#login-to-aws-ec2-remote-server","text":"In this part, we\u2019ll use the video tutorials and the information below to log in and set up a remote script editor. Open a terminal and login like this: ssh -i path/to/key/key_<username>.pem <username>@<IP> Warning change path/to/key to the actual path where you have put the key file. replace and with your actual username and IP","title":"Login to AWS EC2 remote server"},{"location":"course_material/server_login/#setup-your-atom-and-filezilla","text":"","title":"Setup your Atom and FileZilla"},{"location":"course_material/server_login/#atom","text":"Atom is a versatile text editor for all major operating systems. For this course, it\u2019s the recommended script editor for Linux and Mac OS users. With the third-party package ftp-remote-edit , you can remotely edit scripts. Set it up on your own computer using your own credentials and the video below. In general, setup the connection to the server with the following details: protocol: sftp username: your username hostname: server IP port: 22 authentication/logon type: path to private key file","title":"Atom"},{"location":"course_material/server_login/#filezilla","text":"Many results come in an image (e.g. .png , .jpg ) or html format. These can not be viewed directly from the server. Also, for this course, files loaded in IGV need to be on your local computer. You can easily transfer files between your local PC and the remote host with FileZilla . Set it up on your own computer using your own credentials and the video below.","title":"FileZilla"},{"location":"course_material/server_login/#initiate-conda","text":"To make use of the pre-installed software with conda, we need to initiate it first. Login to the server and run: /opt/miniconda3/bin/conda init exec bash Now, your shell should start with (base) , meaning that the conda base environment is loaded. To load the environment longreads with the required software packages, run: conda activate longreads Which should change the start of your shell from (base) to (longreads) Activating the environment You will need to activate the longreads environment each time you login. Windows","title":"Initiate conda"},{"location":"course_material/server_login/#material_1","text":"You have received an e-mail shortly before the workshop with a key, username and IP address to login on a cloud server. Below you can find the material that helps you to login, edit scripts and transfer files. Great power comes with great responsibility The cloud server is a temporary instance for this workshop only. Although the computational resources should be more than enough, it\u2019s a basic Ubuntu server, and there are no hard limits on memory or CPU usage. Take therefore into account that great power comes with great responsibility. Overloading it can result in a reboot, cancelling all running calculations.","title":"Material"},{"location":"course_material/server_login/#video-tutorials_1","text":"Set up MobaXterm Set up FileZilla","title":"Video tutorials"},{"location":"course_material/server_login/#exercises_1","text":"","title":"Exercises"},{"location":"course_material/server_login/#set-up-mobaxterm","text":"In this part, you will use the video tutorials and the information below to log in and set up a remote script editor. MobaXterm is an SSH client for Windows. Use this to connect to the remote host and edit remote scripts. With MobaXterm, you will automatically login to the remote server once you\u2019ve started the SSH session. Set it up on your own computer using your own credentials and the video below. These are the general settings you should take into account: protocol: sftp username: your username hostname: server IP port: 22 authentication/logon type: path to private key file","title":"Set up MobaXterm"},{"location":"course_material/server_login/#set-up-filezilla","text":"Many results come in an image (e.g. .png , .jpg ) or html format. These can not be viewed directly from the server. Also, for this course, files loaded in IGV need to be on your local computer. You can easily transfer files between your local PC and the remote host with FileZilla . Set it up on your own computer using your own credentials and the video below.","title":"Set up FileZilla"},{"location":"course_material/server_login/#initiate-conda_1","text":"To make use of the pre-installed software with conda, we need to initiate it first. Login to the server and run: /opt/miniconda3/bin/conda init exec bash Now, your shell should start with (base) , meaning that the conda base environment is loaded. To load the environment longreads with the required software packages, run: conda activate longreads Which should change the start of your shell from (base) to (longreads) Activating the environment You will need to activate the longreads environment each time you login. Docker","title":"Initiate conda"},{"location":"course_material/server_login/#material_2","text":"Instructions to install docker Instructions to set up to container","title":"Material"},{"location":"course_material/server_login/#exercises_2","text":"","title":"Exercises"},{"location":"course_material/server_login/#first-login","text":"Docker can be used to run an entire isolated environment in a container. This means that we can run the software with all its dependencies required for this course locally in your computer. Independent of your operating system. In the video below there\u2019s a tutorial on how to set up a docker container for this course. Note that you will need administrator rights, and that if you are using Windows, you need the latest version of Windows 10. The command to run the environment required for this course looks like this (in a terminal or powershell): Modify the script Modify the path after -v to the working directory on your computer before running it. Mac OS/Linux terminal docker run \\ -v /full/path/to/local/workdir:/root/workdir \\ -i -t \\ geertvangeest/ngs-longreads \\ /bin/bash Windows powershell docker run ` -v C : \\ Users \\ myusername : / root / workdir ` -i -t ` geertvangeest / ngs-longreads ` / bin / bash The option -v mounts a local directory in your computer to the directory /root/workdir in the docker container. In that way, you have files available both in the container and on your computer. Use this directory on your computer to e.g. edit scripts and visualise data with IGV. Change the first path to a path on your computer that you want to use as a working directory. Don\u2019t mount directly in the home dir Don\u2019t directly mount your local directory to the home directory ( /root ). This will lead to unexpected behaviour. The options -i and -t let you approach the container interactively. Meaning that you can use the shell. The part geertvangeest/ngs-longreads is the image we are going to load into the container. The image contains all the information about software and dependencies needed for this course. When you run this command for the first time it will download the image. Once it\u2019s on your computer, it will start immediately. The last bit /bin/bash tells us which entrypoint we take. Which is the bash command line interpreter. You can exit the shell with exit .","title":"First login"},{"location":"course_material/server_login/#working-with-a-running-container","text":"","title":"Working with a running container"},{"location":"course_material/server_login/#restarting","text":"After exiting, you can restart the container. Find the container name: docker container ls -a The name is e.g. adoring_bell . To restart run: docker start adoring_bell docker attach adoring_bell","title":"Restarting"},{"location":"course_material/server_login/#second-shell","text":"If you want to have a second shell in your container, e.g. because your current shell is busy, you can use: docker exec -it adoring_bell /bin/bash Difference docker attach and docker exec Difference between the commands is explainer here . Conclusion: do not run docker attach for a second shell in which you usually want to start a new process.","title":"Second shell"},{"location":"course_material/server_login/#lost-the-container","text":"If you lost the container for whatever reason, no problem. If you did all your work in the mounted workdir, you can just remount it to a new container based on the same image. To do that, just rerun the docker run command (with the option -v , -i , -t and the entrypoint).","title":"Lost the container"},{"location":"course_material/server_login/#save-your-own-version","text":"If you have additional installations, and you want to keep them, you can save the image with: docker commit adoring_bell my-image conda If you have a conda installation on your local computer, you can install the required software using conda. You can build the environment from ngs-longreads.yml Generate the conda environment like this: conda env create --name ngs-longreads -f ngs-longreads.yml The yaml file probably only works for Linux systems If you want to use the conda environment on a different OS, use: conda create -n ngs-longreads python = 3 .6 conda activate ngs-longreads conda install -y -c bioconda \\ samtools \\ minimap2 \\ fastqc \\ pbmm2 \\ conda install -y -c bioconda nanoplot If the installation of NanoPlot fails, try to install it with pip : pip install NanoPlot This will create the conda environment ngs-longreads Activate it like so: conda activate ngs-longreads After successful installation and activating the environment all the software required to do the exercises should be available.","title":"Save your own version"},{"location":"course_material/server_login/#a-unix-command-line-interface-cli-refresher","text":"Most bioinformatics software are UNIX based and are executed through the CLI. When working with NGS data, it is therefore convenient to improve your knowledge on UNIX. For this course, we need basic understanding of UNIX CLI, so here are some exercises to refresh your memory.","title":"A UNIX command line interface (CLI) refresher"},{"location":"course_material/server_login/#make-a-new-directory","text":"Login to the server and use the command line to make a directory called workdir . If working with Docker If your are working with docker you are a root user. This means that your \u201chome\u201d directory is the root directory, i.e. /root , and not /home/username . If you have mounted your local directory to /root/workdir , this directory should already exist. Answer cd mkdir workdir Make a directory scripts within ~/workdir and make it your current directory. Answer cd workdir mkdir scripts cd scripts","title":"Make a new directory"},{"location":"course_material/server_login/#file-permissions","text":"Generate an empty script in your newly made directory ~/workdir/scripts like this: touch new_script.sh Add a command to this script that writes \u201cSIB courses are great!\u201d (or something you can better relate to.. ) to stdout, and try to run it. Answer You can use your remote script editor to edit your script. Otherwise you can use nano to edit it: nano new_script.sh The script should look like this: #!/usr/bin/env bash echo \"SIB courses are great!\" Usually, you can run it like this: ./new_script.sh But there\u2019s an error: bash: ./new_script.sh: Permission denied Why is there an error? Hint Use ls -lh new_script.sh to check the permissions. Answer ls -lh new_script.sh gives: -rw-r--r-- 1 user group 51B Nov 11 16 :21 new_script.sh There\u2019s no x in the permissions string. You should change at least the permissions of the user. Make the script executable for yourself, and run it. Answer Change permissions: chmod u+x new_script.sh ls -lh new_script.sh now gives: -rwxr--r-- 1 user group 51B Nov 11 16:21 new_script.sh So it should be executable: ./new_script.sh More on chmod and file permissions here .","title":"File permissions"},{"location":"course_material/server_login/#redirection-and","text":"In the root directory (go there like this: cd / ) there are a range of system directories and files. Write the names of all directories and files to a file called system_dirs.txt in your home directory (use ls and > ). Answer ls / > ~/system_dirs.txt The command wc -l counts the number of lines, and can read from stdin. Make a one-liner with a pipe | symbol to find out how many system directories and files there are. Answer ls / | wc -l","title":"Redirection: &gt; and |"},{"location":"course_material/server_login/#variables","text":"Store system_dirs.txt as variable (like this: VAR=variable ), and use wc -l on that variable to count the number of lines in the file. Answer FILE = system_dirs.txt wc -l $FILE","title":"Variables"},{"location":"course_material/server_login/#shell-scripts","text":"Make a shell script that automatically counts the number of system directories and files. Answer Make a script called e.g. current_system_dirs.sh : #!/usr/bin/env bash cd / ls | wc -l","title":"shell scripts"},{"location":"course_material/server_login/#3-loops","text":"20 minutes If you want to run the same command on a range of arguments, it\u2019s not very convenient to type the command for each individual argument. For example, you could write dog , fox , bird to stdout in a script like this: #!/usr/bin/env bash echo dog echo fox echo bird However, if you want to change the command (add an option for example), you would have to change it for all the three command calls. Amongst others for that reason, you want to write the command only once. You can do this with a for-loop, like this: #!/usr/bin/env bash ANIMALS = \"dog fox bird\" for animal in $ANIMALS do echo $animal done Which results in: dog fox bird Write a shell script that removes all the letters \u201ce\u201d from a list of words. Hint Removing the letter \u201ce\u201d from a string can be done with tr like this: word = \"test\" echo $word | tr -d \"e\" Which would result in: tst Answer Your script should e.g. look like this (I\u2019ve added some awesome functionality): #!/usr/bin/env bash WORDLIST = \"here is a list of words resulting in a sentence\" for word in $WORDLIST do echo \"' $word ' with e's removed looks like:\" echo $word | tr -d \"e\" done resulting in: 'here' with e's removed looks like: hr 'is' with e's removed looks like: is 'a' with e's removed looks like: a 'list' with e's removed looks like: list 'of' with e's removed looks like: of 'words' with e's removed looks like: words 'resulting' with e's removed looks like: rsulting 'in' with e's removed looks like: in 'a' with e's removed looks like: a 'sentence' with e's removed looks like: sntnc","title":"3. Loops"}]}